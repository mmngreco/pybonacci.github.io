<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="author" content="Pybonacci">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">
        <title>Introducci√≥n a Machine Learning con Python (Parte 2) | Pybonacci</title>

	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/favicon.ico" type="image/x-icon">
        <link rel="alternate" type="application/atom+xml" title="Pybonacci blog atom feed" href="/feeds/all.atom.xml" />
        <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700' rel='stylesheet' type='text/css'>

        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link rel="stylesheet" type="text/css" href="/theme/css/fontello.css"/>
        <style>.highlight .hll { background-color: #ffffcc }
.highlight .c { color: #60a0b0; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #007020; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .cm { color: #60a0b0; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #007020 } /* Comment.Preproc */
.highlight .c1 { color: #60a0b0; font-style: italic } /* Comment.Single */
.highlight .cs { color: #60a0b0; background-color: #fff0f0 } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #808080 } /* Generic.Output */
.highlight .gp { color: #c65d09; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0040D0 } /* Generic.Traceback */
.highlight .kc { color: #007020; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #007020; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #007020; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #007020 } /* Keyword.Pseudo */
.highlight .kr { color: #007020; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #902000 } /* Keyword.Type */
.highlight .m { color: #40a070 } /* Literal.Number */
.highlight .s { color: #4070a0 } /* Literal.String */
.highlight .na { color: #4070a0 } /* Name.Attribute */
.highlight .nb { color: #007020 } /* Name.Builtin */
.highlight .nc { color: #0e84b5; font-weight: bold } /* Name.Class */
.highlight .no { color: #60add5 } /* Name.Constant */
.highlight .nd { color: #555555; font-weight: bold } /* Name.Decorator */
.highlight .ni { color: #d55537; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #007020 } /* Name.Exception */
.highlight .nf { color: #06287e } /* Name.Function */
.highlight .nl { color: #002070; font-weight: bold } /* Name.Label */
.highlight .nn { color: #0e84b5; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #062873; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #bb60d5 } /* Name.Variable */
.highlight .ow { color: #007020; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mf { color: #40a070 } /* Literal.Number.Float */
.highlight .mh { color: #40a070 } /* Literal.Number.Hex */
.highlight .mi { color: #40a070 } /* Literal.Number.Integer */
.highlight .mo { color: #40a070 } /* Literal.Number.Oct */
.highlight .sb { color: #4070a0 } /* Literal.String.Backtick */
.highlight .sc { color: #4070a0 } /* Literal.String.Char */
.highlight .sd { color: #4070a0; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #4070a0 } /* Literal.String.Double */
.highlight .se { color: #4070a0; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #4070a0 } /* Literal.String.Heredoc */
.highlight .si { color: #70a0d0; font-style: italic } /* Literal.String.Interpol */
.highlight .sx { color: #c65d09 } /* Literal.String.Other */
.highlight .sr { color: #235388 } /* Literal.String.Regex */
.highlight .s1 { color: #4070a0 } /* Literal.String.Single */
.highlight .ss { color: #517918 } /* Literal.String.Symbol */
.highlight .bp { color: #007020 } /* Name.Builtin.Pseudo */
.highlight .vc { color: #bb60d5 } /* Name.Variable.Class */
.highlight .vg { color: #bb60d5 } /* Name.Variable.Global */
.highlight .vi { color: #bb60d5 } /* Name.Variable.Instance */
.highlight .il { color: #40a070 } /* Literal.Number.Integer.Long */</style>
        <style>.description-author {
  font-size: 0.8em;
  color: #333;
}
.img-author {
  max-height: 10em;
  max-width: 10em;
}
body {
  margin: 0;
  padding: 0;
  font: 15px 'Source Sans Pro', sans-serif;
  line-height: 1.6em;
  color: #222222;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
}
a {
  color: #007ee5;
  text-decoration: none;
}
a:hover {
  color: #007ee5;
  text-decoration: none;
}
header.main-header {
  background: none repeat scroll 0% 0% #205F29;
  margin-bottom: 0px;
}
header.main-header a {
  color: #fff;
}
header.main-header .container {
  max-width: 1000px;
}
header.main-header .container nav a:hover {
  background-color: #5C881C;
}
header.navbar-default {
  border-bottom: none;
  background-color: #EFEFEF;
}
article {
  margin: 0;
}
article header.about {
  margin-bottom: 0px;
  padding-bottom: 0px;
}
article header {
  padding-bottom: 20px;
}
article header h1 {
  margin-bottom: 2px;
  font-weight: 700;
  color: #000;
}
article header time {
  color: #9E9E9E;
  float: right;
}
article header time.left {
  color: #9E9E9E;
  float: left;
}
article div.social-links ul {
  padding: 0px;
}
article div.social-links li {
  display: inline;
  font-size: 20px;
}
article div.social-links li a {
  color: #000;
  padding: 10px;
}
article div.social-links li a:hover {
  color: #666;
  text-decoration: none;
}
article p {
  font-size: 2em;
  margin-bottom: 20px;
  line-height: 1.6em;
  text-align: justify;
}
article p.note {
  background: #f5f5f5;
  border: 1px solid #ddd;
  padding: 0.533em 0.733em;
}
article p.update {
  background-color: #FEEFB3;
  border: 1px solid #e6e68a;
  padding: 0.533em 0.733em;
}
article p.alert {
  background-color: #ffe2e2;
  border: 1px solid #ffb2b2;
  padding: 0.533em 0.733em;
}
article ul,
article ol {
  margin-top: 0px;
  margin-bottom: 25px;
}
article li {
  font-size: 16px;
  line-height: 1.6em;
}
article a:hover {
  text-decoration: underline;
}
article blockquote {
  border-left: 2px solid #c7c7cc;
  color: #666;
  margin: 30px 0;
  padding: 0 0 0 25px;
}
article img {
  max-width: 100%;
}
article code {
  color: #333;
  background-color: #EEE;
  border-radius: 0;
  font-size: 1em;
}
article .meta {
  margin-top: 35px;
}
article .meta a:hover {
  text-decoration: none;
}
article .meta div {
  display: block;
}
article .meta address:before,
article .meta time:before,
article .meta a.tag:before {
  font-family: 'fontello';
  margin-right: 6px;
}
article .meta address.author {
  float: left;
}
article .meta address:before {
  content: '\e819';
}
article .meta time:before {
  content: '\f133';
}
article .meta div.tags {
  clear: both;
}
article .meta a.tag {
  margin: 0 10px 10px 0;
  padding: 1px 12px;
  display: inline-block;
  font-size: 14px;
  color: rgba(0, 0, 0, 0.8);
  background: rgba(0, 0, 0, 0.05);
}
article .meta a.tag:before {
  content: '\e821';
}
article .meta a.tag:hover {
  background: rgba(0, 0, 0, 0.15);
}
article .meta a.read_more,
article .meta a.comments_btn {
  font-size: 14px;
  font-weight: 800;
  padding: 10px 20px;
  color: #007aa3;
  background: #FFF;
  border: 1px solid #007aa3;
}
article .meta a.read_more:hover,
article .meta a.comments_btn:hover {
  color: #FFF;
  background: #007aa3;
}
article .meta:after {
  content: "";
  display: table;
  clear: both;
}
.index {
  max-width: 700px;
}
.index article header h2 {
  font-size: 36px;
  margin-bottom: 2px;
  font-weight: 700;
}
.index article header h2 a {
  color: #333;
}
.index article header h2 a:hover {
  color: #007ee5;
  text-decoration: none;
}
.index .separator {
  padding: 40px 0 0 0;
  margin: 0 0 40px 0;
  height: 10px;
  border-bottom: solid 1px #CCC;
}
.index .pagination {
  display: block;
  margin-bottom: 100px;
}
.index .pagination .left {
  text-align: right;
}
.index .pagination .right {
  text-align: left;
}
.index .pagination a {
  display: inline-block;
  border: 2px solid #5C881C;
  margin: 0 5px;
  padding: 8px 20px;
  font-weight: bold;
  color: #5C881C;
}
.index .pagination a:hover {
  color: #FFF;
  background: #5C881C;
}
.post {
  max-width: 80%;
}
.post h1 {
  font-size: 42px;
}
.post h2:before {
  content: "# ";
  font-weight: bold;
  color: #DDD;
}
.post h3:before {
  content: "## ";
  font-weight: bold;
  color: #DDD;
}
.post h4:before {
  content: "### ";
  font-weight: bold;
  color: #DDD;
}
.post p {
  font-size: 2em;
}
.list {
  max-width: 700px;
}
.list ul.double-list {
  margin: 0 auto 60px;
  padding: 0;
  list-style-type: none;
}
.list ul.double-list li {
  padding: 5px 0;
}
.list ul.double-list li h2 {
  font-size: 2em;
  display: inline;
  font-weight: normal;
}
.list ul.double-list li span {
  font-family: sans-serif;
  text-transform: uppercase;
  text-align: right;
  float: right;
  padding-top: 3px;
  font-size: 12px;
  color: #999;
}
.full-width-content {
  padding-top: 10px;
  padding-left: 0px;
  padding-right: 0px;
  margin-left: -20px;
  margin-right: -20px;
}
.col-xs-1,
.col-sm-1,
.col-md-1,
.col-lg-1,
.col-xs-2,
.col-sm-2,
.col-md-2,
.col-lg-2,
.col-xs-3,
.col-sm-3,
.col-md-3,
.col-lg-3,
.col-xs-4,
.col-sm-4,
.col-md-4,
.col-lg-4,
.col-xs-5,
.col-sm-5,
.col-md-5,
.col-lg-5,
.col-xs-6,
.col-sm-6,
.col-md-6,
.col-lg-6,
.col-xs-7,
.col-sm-7,
.col-md-7,
.col-lg-7,
.col-xs-8,
.col-sm-8,
.col-md-8,
.col-lg-8,
.col-xs-9,
.col-sm-9,
.col-md-9,
.col-lg-9,
.col-xs-10,
.col-sm-10,
.col-md-10,
.col-lg-10,
.col-xs-11,
.col-sm-11,
.col-md-11,
.col-lg-11,
.col-xs-12,
.col-sm-12,
.col-md-12,
.col-lg-12 {
  padding-right: 0px;
  padding-left: 0px;
}
.disclaimer {
  text-align: center;
  background-color: #EFEFEF;
  border-bottom: none;
  margin-top: 6em;
}</style>

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>

        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?..." type="text/javascript"></script>
        <script type="text/javascript">
        init_mathjax = function() {
            if (window.MathJax) {
                // MathJax loaded
                MathJax.Hub.Config({
                    tex2jax: {
                        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                        displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
                    },
                    displayAlign: 'left', // Change this to 'center' to center equations.
                    "HTML-CSS": {
                        styles: {'.MathJax_Display': {"margin": 0}}
                    }
                });
                MathJax.Hub.Queue(["Typeset",MathJax.Hub]);
            }
        }
        init_mathjax();
        </script>

    </head>

    <body>
        <header class="navbar navbar-default bs-docs-nav">
            <div class="container-fluid">
                <div class="navbar-header">
		  <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#theNavbar">
		    <span class="icon-bar"></span>
		    <span class="icon-bar"></span>
		    <span class="icon-bar"></span> 
		  </button>
                  <a class="navbar-brand" href="/" title="Home" class="title">Pybonacci</a><!-- ‚Äî Python y Ciencia-->
                </div>
                <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation" id="theNavbar">
		    <ul class="nav navbar-nav navbar-right">
                            <li><a href="/pages/acerca-de-pybonacci.html" title="About">Acerca de</a></li>
                            <li><a href="/archives.html" title="Archive">Archivos</a></li>
                            <li><a class="nodec icon-rss" href="/feeds/all.atom.xml" title="pybonacci.github.io RSS feed" rel="me"></a></li>
                    </ul>
                </nav>
            </div>
        </header>

        <div id="wrap">
<div class="container post">
    <article>
        <header>
            <h1>Introducci√≥n a Machine Learning con Python (Parte 2)</h1>
            <div class="meta">
                <time datetime="article.date.isoformat()" pubdate>lun 06 abril 2015</time>
                <address class="vcard author">Por
                    <a class="url fn" href="http://pybonacci.github.io/author/pablo-fernandez.html">Pablo Fern√°ndez</a>
                </address>
            </div>
        </header>

        <div class="article_content">
            <p>En la entrada anterior, <a href="http://pybonacci.org/2015/01/14/introduccion-a-machine-learning-con-python-parte-1/" title="Introducci√≥n a Machine Learning con Python (Parte 1)">Introducci√≥n a Machine Learning con Python (Parte 1)</a>, di unas peque√±as pinceladas sobre lo que es el Aprendizaje Autom√°tico con algunos ejemplos pr√°cticos. Ahora vamos a adentrarnos en materia de un modo m√°s estructurado viendo paso a paso algunas de las t√©cnicas que podemos emplear en Python.</p>
<p>Podemos dividir los problemas de aprendizaje autom√°tico en dos grandes categor√≠as (Pedregosa et al., 2011):</p>
<ul>
<li><strong>Aprendizaje supervisado</strong>, cuando el conjunto de datos viene con los atributos adicionales que queremos predecir. El problema puede clasificarse en dos categor√≠as: <ul>
<li>Regresi√≥n: los valores de salida consisten en una o m√°s variables continuas. Un ejemplo es la predicci√≥n del valor de una casa en funci√≥n de su superficie √∫til, n√∫mero de habitaciones, cuartos de ba√±os, etc.</li>
<li>Clasificaci√≥n: las muestras pertenecen a dos o m√°s clases y queremos aprender a partir de lo que ya conocemos c√≥mo clasificar nuevas muestras. Tenemos como ejemplo el <em>Iris dataset</em> que ya mostramos en la entrada anterior</li>
</ul>
</li>
<li><strong>Aprendizaje no supervisado</strong>, cuando no hay un conocimiento a priori de las salidas que corresponden al conjunto de datos de entrada. En estos casos el objetivo es encontrar grupos mediante <em><a title="Cluster analysis" href="http://en.wikipedia.org/wiki/Cluster_analysis" target="_blank">clustering</a></em> o determinar una distribuci√≥n de probabilidad sobre un conjunto de entrada.</li>
</ul>
<p>Como vemos, en ambos casos el aprendizaje autom√°tico trata de aprender una serie de propiedades del conjunto de datos y aplicarlos a nuevos datos.</p>
<p>√âsta entrada se la vamos a dedicar al aprendizaje supervisado, acompa√±ando cada una de las t√©cnica que veamos con un Notebook de <a title="Jupyter" href="http://jupyter.org" target="_blank">Jupyter</a>.</p>
<h2>Aprendizaje supervisado</h2>
<p>Empezaremos por el principio, y lo m√°s sencillo, que es ajustar los datos a una l√≠nea para pasar luego a ver diferentes modelos de clasificaci√≥n en orden creciente de complejidad en subsiguientes entradas.</p>
<!--more Sigue leyendo...-->

<h3><a href="http://nbviewer.ipython.org/github/pybonacci/notebooks/blob/master/Machine%20Learning/Regresi√≥n%20lineal.ipynb" target="_blank">Regresi√≥n lineal</a></h3>
<p>Los modelos lineales son fundamentales tanto en estad√≠stica como en el aprendizaje autom√°tico, pues muchos m√©todos se apoyan en la combinaci√≥n lineal de variables que describen los datos. Lo m√°s sencillo es ajustar una l√≠nea recta con <code>LinearRegression</code>.</p>
<p>Para mostrar c√≥mo funcionan estos modelos vamos a emplear uno de los <a title="Dataset loading utilities" href="http://scikit-learn.org/stable/datasets/index.html" target="_blank">dataset</a> que ya incorpora scikit-learn.</p>
<pre><code class="language-python">from sklearn import datasets
boston = datasets.load_boston()</code></pre>

<p>El Boston dataset es un conjunto de datos para el an√°lisis de los precios de las viviendas en la regi√≥n de Boston. Con <code>boston.DESCR</code> podemos obtener una descripci√≥n del dataset, con informaci√≥n sobre el mismo, como el tipo de atributos.</p>
<div class="highlight"><pre><span></span><span class="n">Boston</span> <span class="n">House</span> <span class="n">Prices</span> <span class="n">dataset</span>
<span class="n">Notes</span>
------
<span class="n">Data</span> <span class="nb">Set</span> <span class="n">Characteristics:</span>   
    :<span class="n">Number</span> <span class="k">of</span> <span class="n">Instances:</span> <span class="mi">506</span> 
    :<span class="n">Number</span> <span class="k">of</span> <span class="n">Attributes:</span> <span class="mi">13</span> <span class="n">numeric</span><span class="o">/</span><span class="n">categorical</span> <span class="n">predictive</span>
    :<span class="n">Median</span> <span class="n">Value</span> (<span class="n">attribute</span> <span class="mi">14</span>) <span class="k">is</span> <span class="n">usually</span> <span class="n">the</span> <span class="n">target</span>
    :<span class="n">Attribute</span> <span class="n">Information</span> (<span class="n">in</span> <span class="n">order</span>):
    - <span class="n">CRIM</span>     <span class="n">per</span> <span class="n">capita</span> <span class="n">crime</span> <span class="n">rate</span> <span class="nb">by</span> <span class="n">town</span>
    - <span class="n">ZN</span>       <span class="n">proportion</span> <span class="k">of</span> <span class="n">residential</span> <span class="n">land</span> <span class="n">zoned</span> <span class="k">for</span> <span class="n">lots</span> <span class="n">over</span> <span class="mi">25</span>,<span class="mo">000</span> <span class="n">sq</span>.<span class="n">ft</span>.
    - <span class="n">INDUS</span>    <span class="n">proportion</span> <span class="k">of</span> <span class="n">non-retail</span> <span class="n">business</span> <span class="n">acres</span> <span class="n">per</span> <span class="n">town</span>
    - <span class="n">CHAS</span>     <span class="n">Charles</span> <span class="n">River</span> <span class="n">dummy</span> <span class="n">variable</span> (= <span class="mi">1</span> <span class="k">if</span> <span class="n">tract</span> <span class="n">bounds</span> <span class="n">river</span>; <span class="mi">0</span> <span class="n">otherwise</span>)
    - <span class="n">NOX</span>      <span class="n">nitric</span> <span class="n">oxides</span> <span class="n">concentration</span> (<span class="n">parts</span> <span class="n">per</span> <span class="mi">10</span>  <span class="n">million</span>)
    - <span class="n">RM</span>       <span class="n">average</span> <span class="n">number</span> <span class="k">of</span> <span class="n">rooms</span> <span class="n">per</span> <span class="n">dwelling</span>
    - <span class="n">AGE</span>      <span class="n">proportion</span> <span class="k">of</span> <span class="n">owner-occupied</span> <span class="n">units</span> <span class="n">built</span> <span class="n">prior</span> <span class="nb">to</span> <span class="mi">1940</span>
    - <span class="n">DIS</span>      <span class="n">weighted</span> <span class="n">distances</span> <span class="nb">to</span> <span class="n">five</span> <span class="n">Boston</span> <span class="n">employment</span> <span class="n">centres</span>
    - <span class="n">RAD</span>      <span class="nb">index</span>  <span class="k">of</span> <span class="n">accessibility</span> <span class="nb">to</span> <span class="n">radial</span> <span class="n">highways</span>
    - <span class="n">TAX</span>      <span class="n">full-value</span> <span class="n">property-tax</span>  <span class="n">rate</span> <span class="n">per</span> <span class="nv">$10</span>,<span class="mo">000</span>
    - <span class="n">PTRATIO</span>  <span class="n">pupil-teacher</span> <span class="n">ratio</span> <span class="nb">by</span> <span class="n">town</span>
    - <span class="n">B</span>        <span class="mi">1000</span>(<span class="n">Bk</span> - <span class="mf">0.63</span>)^<span class="mi">2</span> <span class="k">where</span> <span class="n">Bk</span> <span class="k">is</span> <span class="n">the</span> <span class="n">proportion</span> <span class="k">of</span> <span class="n">blacks</span> <span class="nb">by</span> <span class="n">town</span>
    - <span class="n">LSTAT</span>    % <span class="n">lower</span> <span class="n">status</span> <span class="k">of</span> <span class="n">the</span> <span class="n">population</span>
    - <span class="n">MEDV</span>     <span class="n">Median</span> <span class="nb">value</span> <span class="k">of</span> <span class="n">owner-occupied</span> <span class="n">homes</span> <span class="n">in</span> <span class="nv">$1000&#39;s</span>
    :<span class="n">Missing</span> <span class="n">Attribute</span> <span class="n">Values:</span> <span class="n">None</span>
</pre></div>


<p>Vemos que tenemos 506 muestras con 13 atributos que nos ayudar√°n a predecir el precio medio de la vivienda. Ahora bien, no todos los atributos ser√°n significativos ni todos tendr√°n el mismo peso a la hora de determinar el precio de la vivienda; pero eso es algo que iremos viendo conforme adquiramos experiencia e intuici√≥n.</p>
<p>Ya tenemos los datos, vamos a ajustar una l√≠nea recta para ver cu√°l es la tendencia que siguen los precios en funci√≥n del atributo.</p>
<p>Lo primero es importar <code>LinearRegression</code> y crear un objeto.</p>
<pre><code class="language-python">from sklearn.linear_model import LinearRegression
lr = LinearRegression(normalize=True)</code></pre>

<p>Una vez tenemos claro el modelo a emplear, el siguiente paso es entrenarlo con los datos de variables independientes y variables dependientes que tenemos. Para ello, en scikit-learn tenemos funciones del tipo <code>modelo.fit(X, y)</code>.</p>
<pre><code class="language-python">lr.fit(boston.data, boston.target)</code></pre>

<p>√âste, al tratarse de un modelo sencillo y con muy pocas muestra tardar√° muy poco en entrenarse. Una vez completado el proceso podemos ver los coeficientes que ha asignado a cada atributo y as√≠ ver de qu√© manera contribuyen al precio final de la vivienda.</p>
<pre><code class="language-python">for (feature, coef) in zip(boston.feature_names, lr.coef_):
    print('{:&gt;7}: {: 9.5f}'.format(feature, coef))</code></pre>

<div class="highlight"><pre><span></span>CRIM:  -0.10717
     ZN:   0.04640
  INDUS:   0.02086
   CHAS:   2.68856
    NOX: -17.79576
     RM:   3.80475
    AGE:   0.00075
    DIS:  -1.47576
    RAD:   0.30566
    TAX:  -0.01233
PTRATIO:  -0.95346
      B:   0.00939
  LSTAT:  -0.52547
</pre></div>


<p>Con esto ya tendr√≠amos una peque√±a idea de cuales son los factores que m√°s contribuyen a incrementar o disminuir el precio de la vivienda. Pero no vayamos a sacar conclusiones precipitadas como han hecho en su d√≠a <a href="http://www.bbc.com/news/magazine-22223190" target="_blank">Reinhart y Rogoff</a> y visualicemos los datos primero.</p>
<pre><code class="language-python">%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
def plot_feature(feature):
    f = (boston.feature_names == feature)
    plt.scatter(boston.data[:,f], boston.target, c='b', alpha=0.3)
    plt.plot(boston.data[:,f], boston.data[:,f]*lr.coef_[f] + lr.intercept_, 'k')
    plt.legend(['Predicted value', 'Actual value'])
    plt.xlabel(feature)
    plt.ylabel("Median value in $1000's")
plot_feature('AGE')</code></pre>

<p><a href="http://new.pybonacci.org/images/2015/04/age.png"><img src="http://new.pybonacci.org/images/2015/04/age.png" alt="age" width="388" height="271" class="aligncenter size-full wp-image-3325" srcset="https://pybonacci.org/wp-content/uploads/2015/04/age.png 388w, https://pybonacci.org/wp-content/uploads/2015/04/age-300x209.png 300w" sizes="(max-width: 388px) 100vw, 388px" /></a></p>
<p>En este caso hemos representado el precio medio la vivienda frente a la proporci√≥n de viviendas anteriores a 1940 que hay en la zona. Y como poder ver cl√°ramente, emplear s√≥lo un par√°metro (AGE) para determinar el precio de la vivienda mediante una l√≠nea recta no parece lo ideal. Pero si tomamos en cuenta todas las variables las predicciones posiblemente mejoren.</p>
<p>Por tanto vamos a utilizar el modelo ya entrenado para predecir los precios de las viviendas. Aunque en este caso no vamos a utilizar datos nuevos, sino los mismos datos que hemos empleado para entrenar el modelo y as√≠ ver las diferencias.</p>
<pre><code class="language-python">predictions = lr.predict(boston.data)
f, ax = plt.subplots(1)
ax.hist(boston.target - predictions, bins=50, alpha=0.7)
ax.set_title('Histograma de residuales')
ax.text(0.95, 0.90, 'Media de residuales: {:.3e}'.format(np.mean(boston.target - predictions)),
        transform=ax.transAxes, verticalalignment='top', horizontalalignment='right')</code></pre>

<p><a href="http://new.pybonacci.org/images/2015/04/hist.png"><img src="http://new.pybonacci.org/images/2015/04/hist.png" alt="hist" width="370" height="266" class="aligncenter size-full wp-image-3327" srcset="https://pybonacci.org/wp-content/uploads/2015/04/hist.png 370w, https://pybonacci.org/wp-content/uploads/2015/04/hist-300x215.png 300w" sizes="(max-width: 370px) 100vw, 370px" /></a></p>
<p>Podemos ver que el error medio es despreciable y que la mayor√≠a de los valores se concentran entorno al 0. Pero, ¬øc√≥mo hemos llegado a esos valores?</p>
<p>La idea detr√°s de la regresi√≥n lineal es encontrar unos coeficientes $\beta$ que satisfagan</p>
<p>$$y = X\beta,$$</p>
<p>donde $X$ es nuestra matriz de datos e $y$ son nuestros valores objetivo. Puesto que es muy poco probable que a partir de nuestros valores de $X$ obtengamos los coeficientes que plenamente satisfagan la ecuaci√≥n, es necesario a√±adir un t√©rmino de error $\varepsilon$, tal que</p>
<p>$$y = X\beta + \varepsilon.$$</p>
<p>Con el fin de obtener ese conjunto de coeficientes $\beta$ que relacionan $X$ con $y$, <code>LinearRegression</code> recurre al m√©todo de m√≠nimos cuadrados</p>
<p>$$\underset{\beta}{min\,} {|| X \beta - y||_2}^2.$$</p>
<p>Para √©ste problema tambi√©n existe una soluci√≥n anal√≠tica,</p>
<p>$$\beta = (X^T X)^{-1}X^Ty,$$</p>
<p>pero, ¬øqu√© ocurre si nuestros datos no son independientes? En ese caso, $X^T X$ no es invertible y si contamos con columnas que son funci√≥n de otras, o est√°n de alguna manera correlacionadas, la estimaci√≥n por m√≠nimos cuadrados se vuelve altamente sensible a errores aleatorios increment√°ndose la varianza.</p>
<h4>Regularizaci√≥n</h4>
<p>Para esos casos emplearemos el modelo <code>Ridge</code> que a√±ade un factor de regularizaci√≥n $\alpha$ que en espa√±ol se conoce como <a href="http://es.wikipedia.org/wiki/Regularizaci√≥n_de_T√≠jonov" target="_blank">factor de T√≠jinov</a>.</p>
<p>$$\underset{\beta}{min\,} {{|| X \beta - y||_2}^2 + \alpha {||\beta||_2}^2},$$</p>
<p>y as√≠ la soluci√≥n anal√≠tica queda como</p>
<p>$$\beta = (X^T X + \alpha^2I)^{-1}X^Ty.$$</p>
<p>Veamos un ejemplo. Para ello, en vez de cargar un dataset crearemos nosotros uno con tres atributos, y donde s√≥lo dos sean linealmente independientes. Para ello utilizamos la funci√≥n <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html" target="_blank"><code>make_regression</code></a>.</p>
<pre><code class="language-python">from sklearn.datasets import make_regression
reg_data, reg_target = make_regression(n_samples=2000, n_features=3, effective_rank=2, noise=10)</code></pre>

<p>Nos interesar√° tambi√©n optimizar el valor de $\alpha$. Eso lo haremos con la validaci√≥n cruzada mediante el objeto <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html" target="_blank"><code>RidgeCV</code></a> que emplea una t√©cnica similar al <em>leave-one-out cross-validation</em> (LOOCV), i.e., dejando uno fuera para test mientras entrena con el resto de las muestras.</p>
<pre><code class="language-python">from sklearn.linear_model import RidgeCV</code></pre>

<p>A la hora de crear el objeto le vamos a indicar los valores de $\alpha$ a evaluar. Tambi√©n guardamos los datos que obtenemos al realizar la validaci√≥n cruzada con <code>store_cv_values=True</code> para representarlos gr√°ficamente.</p>
<pre><code class="language-python"># creamos un numpy array con los valores de alpha que queremos evaluar
alphas = np.linspace(0.01, 0.5)
# que pasamos a nuestro modelo RidgeCV, guardando los resultados
rcv = RidgeCV(alphas=alphas, store_cv_values=True)
rcv.fit(reg_data, reg_target)
# representamos gr√°ficamente el error en funci√≥n de alpha
plt.rc('text', usetex=False)
f, ax = plt.subplots()
ax.plot(alphas, rcv.cv_values_.mean(axis=0))
ax.text(0.05, 0.90, 'alpha que minimiza el error: {:.3f}'.format(rcv.alpha_),
        transform=ax.transAxes)</code></pre>

<p><a href="http://new.pybonacci.org/images/2015/04/ridgecv.png"><img src="http://new.pybonacci.org/images/2015/04/ridgecv.png" alt="ridgecv" width="381" height="251" class="aligncenter size-full wp-image-3339" srcset="https://pybonacci.org/wp-content/uploads/2015/04/ridgecv.png 381w, https://pybonacci.org/wp-content/uploads/2015/04/ridgecv-300x197.png 300w" sizes="(max-width: 381px) 100vw, 381px" /></a></p>
<p>Con <code>rcv.alpha_</code> obtenemos el valor de $\alpha$ que nuestro m√©todo <code>RidgeCV</code> ha considerado minimiza el error, lo cual tambi√©n acabamos de comprobar gr√°ficamente.</p>
<p>Pero m√©todos para regresi√≥n lineal hay muchos, y en la <a href="http://scikit-learn.org/stable/modules/linear_model.html#" target="_blank">documentaci√≥n de scikit-learn</a> podr√©is encontrar una descripci√≥n bastante completa de cada alternativa.</p>
<h3><a href="http://nbviewer.ipython.org/github/pybonacci/notebooks/blob/master/Machine%20Learning/Regresi√≥n%20lineal.ipynb#Regresi√≥n-no-lineal" target="_blank">Regresi√≥n no lineal</a></h3>
<p>Ahora bien, ¬øqu√© hacer cuando la relaci√≥n no es lineal y creemos que un polinomio har√≠a un mejor ajuste? Si tomamos como ejemplo una funci√≥n $f$ que toma la forma</p>
<p>$$f(x) = a + bx + cx^2 $$</p>
<p>la funci√≥n $f$ es no lineal en funci√≥n de $x$ pero si es lineal en funci√≥n de los par√°metros desconocidos $a$, $b$, y $c$. O visto de otra manera: podemos sustituir nuestras variables $x$ por un array $z$ tal que</p>
<p>$$ z = [1, x, x^2] $$</p>
<p>con el que podr√≠amos reescribir nuestra funci√≥n $f$ como</p>
<p>$$ f(z) = az_0 + bz_1 + cz_2$$</p>
<p>Para ello en <code>scikit-learn</code> contamos con la herramienta <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html" target="_blank"><code>PolynomialFeatures</code></a>. Veamos un ejemplo.</p>
<p>En este caso vamos a tomar la funci√≥n <em>seno</em> entre 0 y 2$\pi$ a la que a√±adiremos un poco de ruido.</p>
<pre><code class="language-python">f, ax = plt.subplots()
x = np.linspace(0, 2*np.pi)
y = np.sin(x)
ax.plot(x, np.sin(x), 'r', label='sin ruido')
# a√±adimos algo de ruido
xr = x + np.random.normal(scale=0.1, size=x.shape)
yr = y + np.random.normal(scale=0.2, size=y.shape)
ax.scatter(xr, yr, label='con ruido')
ax.legend()</code></pre>

<p><a href="http://new.pybonacci.org/images/2015/04/seno.png"><img src="http://new.pybonacci.org/images/2015/04/seno.png" alt="seno" width="378" height="256" class="aligncenter size-full wp-image-3340" srcset="https://pybonacci.org/wp-content/uploads/2015/04/seno.png 378w, https://pybonacci.org/wp-content/uploads/2015/04/seno-300x203.png 300w" sizes="(max-width: 378px) 100vw, 378px" /></a></p>
<pre><code class="language-python">from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline</code></pre>

<p>Scikit-learn tiene un objeto <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html" target="_blank"><code>PolynomialFeatures</code></a> que nos va a servir para convertir nuestra variable $x$ en un array $z$ del tipo $z = [1, x, x^2, \ldots, n^n]$, que es lo que nos interesa.</p>
<p>El resultado de esa transformaci√≥n se la pasamos a nuestro modelo <code>Ridge</code>. Para facilitar la tarea en este tipo de casos ‚Äîdonde se realizan varios pasos que van desde el pre-tratamiento de los datos hasta un posible post-tratamiento pasando por el entrenamiento‚Äî, podemos hacer uso de las <a href="http://scikit-learn.org/stable/modules/pipeline.html" target="_blank"><code>Pipeline</code></a> que nos permiten encadenar multiples estimadores en uno. Esto es especialmente √∫til cuando hay secuencia de pasos predefinidos en el procesado de datos con, por ejemplo, selecci√≥n de atributos, normalizaci√≥n y clasificaci√≥n.</p>
<pre><code class="language-python">f, ax = plt.subplots()
ax.plot(x, np.sin(x), 'r', label='sin ruido')
ax.scatter(xr, yr, label='con ruido')
# convertimos nuestro array en un vector columna
X = xr[:, np.newaxis]
# utilizamos un bucle para probar polinomios de diferente grado
for degree in [3, 4, 5]:
    # utilizamos Pipeline para crear una secuencia de pasos
    model = make_pipeline(PolynomialFeatures(degree), Ridge())
    model.fit(X, y)
    y = model.predict(x[:, np.newaxis])
    ax.plot(x, y, '--', lw=2, label="degree %d" % degree)
ax.legend()</code></pre>

<p><a href="http://new.pybonacci.org/images/2015/04/senoridge.png"><img src="http://new.pybonacci.org/images/2015/04/senoridge.png" alt="senoridge" width="378" height="256" class="aligncenter size-full wp-image-3342" srcset="https://pybonacci.org/wp-content/uploads/2015/04/senoridge.png 378w, https://pybonacci.org/wp-content/uploads/2015/04/senoridge-300x203.png 300w" sizes="(max-width: 378px) 100vw, 378px" /></a></p>
<p>Acabamos de utilizar un modelo <code>Ridge</code> que implementa regularizaci√≥n, pero sin optimizar. ¬øQu√© pasar√≠a si optimizamos el par√°metro de regularizaci√≥n $alpha \alpha$ con <code>RidgeCV</code>?</p>
<pre><code class="language-python">f, ax = plt.subplots()
ax.plot(x, np.sin(x), 'r', label='sin ruido')
ax.scatter(xr, yr, label='con ruido')
# convertimos nuestro array en un vector columna
X = xr[:, np.newaxis]
# utilizamos un bucle para probar polinomios de diferente grado
for degree in [3, 4, 5]:
    # utilizamos Pipeline para crear una secuencia de pasos
    model = make_pipeline(PolynomialFeatures(degree), RidgeCV(alphas=alphas))
    model.fit(X, y)
    y = model.predict(x[:, np.newaxis])
    ax.plot(x, y, '--', lw=2, label="degree %d" % degree)
ax.legend()</code></pre>

<p>Si comparamos esta √∫ltima gr√°fica con la anterior vemos que ahora las predicciones se han igualado entre si ofreciendo los polinomios de diferente grado predicciones pr√°cticamente id√©nticas. Eso es porque la regularizaci√≥n tiende a penalizar la complejidad de los modelos tratando de evitar el sobreajuste (<em>overfitting</em>).</p>
<h3><a href="http://nbviewer.ipython.org/github/pybonacci/notebooks/blob/master/Machine%20Learning/Regresi√≥n%20log√≠stica.ipynb" target="_blank">Regresi√≥n log√≠stica</a></h3>
<p>Podemos clasificar de dos formas, mediante discriminaci√≥n o asignando probabilidades. Discriminando, asignamos a cada $x$ una de las $K$ clases $C_k$. Por contra, desde un punto de vista probabil√≠stico, lo que har√≠amos es asignar a cada $x$ la probabilidad de pertenecer a la clase $C_k$. El tipo de clasificaci√≥n que realicemos es a discreci√≥n del usuario y muchas veces depender√° de la distribuci√≥n de los datos o de los requisitos que nos imponga el cliente. Por ejemplo, hay campeonatos en Kaggle donde lo que se pide es identificar la clase ‚Äî<a href="http://www.kaggle.com/c/digit-recognizer/data" target="_blank">Digit Recognizer</a>‚Äî, pero tambi√©n puede ser un requisito el determinar la probabilidad de pertecer a una clase determinada ‚Äî<a href="http://www.kaggle.com/c/otto-group-product-classification-challenge/details/evaluation" target="_blank">Otto Group Product Classification Challenge</a>.</p>
<p>En scikit-learn podemos obtener clasificaciones de ambas maneras una vez entrenado el modelo.</p>
<ul>
<li><code>modelo.predict()</code>, para asignar una categor√≠a.</li>
<li><code>modelo.predict_proba()</code>, para determinar la probabilidad de pertenencia.</li>
</ul>
<p>Aqu√≠ nos centraremos en la parte probabil√≠stica, que espero nos d√© una visi√≥n m√°s ampliar, y a su vez nos servir√° para asignar una categor√≠a si definimos un <a href="http://es.wikipedia.org/wiki/Hiperplano">hiperplano</a>.</p>
<p>Para modelos probabil√≠sticos lo m√°s conveniente, en el caso de contar con dos categor√≠as, es la representaci√≥n binaria donde contamos con una √∫nica variable objetivo $t \in &#123;0,1&#125;$ tal que $t=0$ representa la clase $C_1$ y $t=1$ representa la clase $C_2$. Podemos considerar que el valor de $t$ representa la probabilidad de que la clase sea $C_2$, con los valores de probabilidad tomando valores entre $0$ y $1$.</p>
<p>Veamos un ejemplo.</p>
<pre><code class="language-python">from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
import numpy as np</code></pre>

<p>Con la funci√≥n <code>make_classification</code> de scikit-learn, creamos un conjunto de datos para clasificar. Para empezar vamos a contar con s√≥lo un atributo o </em>feature</em> y dos clases o categor√≠as. Los categor√≠as van a estar separadas, pero permitiremos un cierto grado de solapamiento a trav√©s del par√°metro <code>class_sep</code>; as√≠, la clasificaci√≥n probabil√≠stica cobra m√°s sentido.</p>
<pre><code class="language-python"># con el par√°metro random_state nos aseguramos obtener siempre lo mismo
X, y = make_classification(n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1,
                           class_sep=0.9, random_state=27)
plt.scatter(X, y, alpha=0.4)
plt.xlabel('X')
plt.ylabel('Probabilidad')</code></pre>

<p><a href="http://new.pybonacci.org/images/2015/04/class.png"><img src="http://new.pybonacci.org/images/2015/04/class.png" alt="class" width="392" height="271" class="aligncenter size-full wp-image-3353" srcset="https://pybonacci.org/wp-content/uploads/2015/04/class.png 392w, https://pybonacci.org/wp-content/uploads/2015/04/class-300x207.png 300w" sizes="(max-width: 392px) 100vw, 392px" /></a></p>
<p>En regresi√≥n log√≠stica los que vamos a hacer es calcular las probabilidades $p(C_k|x)$. La funci√≥n log√≠stica o <a href="http://es.wikipedia.org/wiki/Funci√≥n_sigmoide">sigmoide</a> nos va a permitir definir esas probabilidades y viene definida como</p>
<p>$$f(x) = \frac{1}{1 + \exp(-k(x-x_0))} $$</p>
<p>Como veremos a continuaci√≥n, la sigmoide tiene forma de <em>S</em> y la funci√≥n log√≠stica juega un papel muy importante en muchos algoritmos de clasificaci√≥n. Pero no es la √∫nica funci√≥n de ese tipo; tambi√©n podemos encontrarnos las funci√≥n arcotangente, tangente hiperb√≥lica o <a href="http://en.wikipedia.org/wiki/Softmax_function">softmax</a> entre otras.</p>
<p>Como es costumbre en scikit-learn, primero definimos el modelo que vamos a emplear que ser√° <code>LogisticRegression</code>. Lo cargamos con los par√°metros por defecto y lo entrenamos.</p>
<pre><code class="language-python">lr = LogisticRegression()
lr.fit(X, y)</code></pre>

<p>Por defecto, en Jupyter, nos va a imprimir los par√°metros con los que se ha entrenado el modelo. Una vez entrenado podemos predecir las probabilidades de pertenencia a cada categor√≠a. Para ello, como ya hemos dicho, utilizaremos la funci√≥n <code>predict_proba()</code> que toma como datos de entrada los atributos $X$.</p>
<p>Lo que nos devuelve la funci√≥n <code>predict_proba()</code> es un array de dimensiones (n atributos, n clases). A nosotros s√≥lo nos va a interesar representar la segunda columna, es decir, $p(C_1|x)$, pues sabemos que $p(C_1|x) = 1 - p(C_0|x)$.</p>
<pre><code class="language-python">plt.scatter(X, y, alpha=0.4, label='real')
plt.plot(np.sort(X, axis=0), lr.predict_proba(np.sort(X, axis=0))[:,1], color='r', label='sigmoide')
plt.legend(loc=2)
plt.xlabel('X')
plt.ylabel('Probabilidad')</code></pre>

<p><a href="http://new.pybonacci.org/images/2015/04/sigmoide.png"><img src="http://new.pybonacci.org/images/2015/04/sigmoide.png" alt="sigmoide" width="392" height="271" class="aligncenter size-full wp-image-3347" srcset="https://pybonacci.org/wp-content/uploads/2015/04/sigmoide.png 392w, https://pybonacci.org/wp-content/uploads/2015/04/sigmoide-300x207.png 300w" sizes="(max-width: 392px) 100vw, 392px" /></a></p>
<p>Se aprecia claramente la curva en forma de <em>S</em> de la funci√≥n log√≠stica que es lo que est√°bamos buscando. Esto nos dice que un punto con $x=0$ tiene aproximadamente un 50 % de probabilidades de pertenecer a cualquiera de las dos categor√≠as.</p>
<p>Si a partir de las probabilidades quisiesemos hacer una clasificaci√≥n por categor√≠as no tendr√≠amos m√°s que definir un valor umbral. Es decir, cuando la funci√≥n log√≠stica asigna una probabilidad mayor a, por ejemplo, 0.5 entonces asignamos esa categor√≠a. Eso es b√°sicamente lo que hace <code>predict()</code> tal y como podemos ver a continuaci√≥n.</p>
<pre><code class="language-python">plt.scatter(X, y, alpha=0.4, label='real')
plt.plot(np.sort(X, axis=0), lr.predict(np.sort(X, axis=0)), color='r', label='categor√≠a')
plt.legend(loc=2)
plt.xlabel('X')
plt.ylabel('Probabilidad')</code></pre>

<p><a href="http://new.pybonacci.org/images/2015/04/sigmoideumbral.png"><img src="http://new.pybonacci.org/images/2015/04/sigmoideumbral.png" alt="sigmoideumbral" width="392" height="271" class="aligncenter size-full wp-image-3349" srcset="https://pybonacci.org/wp-content/uploads/2015/04/sigmoideumbral.png 392w, https://pybonacci.org/wp-content/uploads/2015/04/sigmoideumbral-300x207.png 300w" sizes="(max-width: 392px) 100vw, 392px" /></a></p>
<h2>Conclusi√≥n</h2>
<p>Los m√©todos que hemos visto en √©sta segunda parte son de los m√°s sencillos que hay, pero son la base fundamental sobre la que se asientan otros m√©todos mucho m√°s complejos. Por ejemplo, dentro de una red neuronal, cada neurona representa una funci√≥n log√≠stica, y el conjunto de ellas es capaz de reconocer objetos en im√°genes. Sencillamente impresionante.</p>
<p>En la tercera entrega de √©sta serie seguiremos viendo m√©todos de aprendizaje supervisado pero con un enfoque ya m√°s pr√°ctico. Tambi√©n veremos c√≥mo competir en Kaggle a trav√©s de sus tutoriales; y una vez hayamos adquirido algo de confianza, pasar a las competiciones. La din√°mica es la misma, lo que cambia es el premio.</p>
<p><a href="http://new.pybonacci.org/images/2015/04/mnist.png"><img src="http://new.pybonacci.org/images/2015/04/mnist.png" alt="mnist" width="684" height="125" class="aligncenter size-full wp-image-3351" srcset="https://pybonacci.org/wp-content/uploads/2015/04/mnist.png 684w, https://pybonacci.org/wp-content/uploads/2015/04/mnist-300x54.png 300w" sizes="(max-width: 684px) 100vw, 684px" /></a></p>
<p>Espero que os haya gustado. Cualquier comentario o sugerencia es bienvenido.</p>
<h2>Referencias</h2>
<p>Bishop, C. (2006).¬†<em>Pattern recognition and machine learning</em>. New York: Springer.</p>
<p>Hauck, T. (2014). <em>Scikit-learn Cookbook</em>. Birmingham, U.K.: Packt Publishing.</p>
<p>Pedregosa, F., Varoquaux, G., Gramfort, A. et al. (2011). Scikit-learn: Machine Learning in Python. <em>Journal of Machine Learning Research</em>, 12, pp. 2825-2830.</p>
<p>Richert, W. and Coelho, L. (2013).¬†<em>Building Machine Learning Systems with Python</em>. Birmingham: Packt Publishing.</p>
        </div>

        <div class="meta">
            <div class="tags">
                    <a href="http://pybonacci.github.io/tag/machine-learning.html" class="tag">machine learning</a>
                    <a href="http://pybonacci.github.io/tag/python.html" class="tag">python</a>
                    <a href="http://pybonacci.github.io/tag/regresion-lineal.html" class="tag">regresi√≥n lineal</a>
                    <a href="http://pybonacci.github.io/tag/regresion-logistica.html" class="tag">regresi√≥n log√≠stica</a>
                    <a href="http://pybonacci.github.io/tag/scikit-learn.html" class="tag">scikit-learn</a>
            </div>
        </div>


    </article>
<script data-isso="//https://comments.pybonacci.org"
        data-isso-lang="es"
        src="//https://comments.pybonacci.org/js/embed.min.js"></script>
<section id="isso-thread">
<h3>Comentarios</h3>
</section>

</div>

<style type="text/css">
{
    max-width: 700px;
}

.text_cell .prompt {
    display: none;
}

div.cell {
    padding: 0;
}

div.text_cell_render {
    padding: 0;
}

div.prompt {
    font-size: 13px;
}

div.input_prompt {
    padding: .7em 0.2em;
}

div.output_prompt {
    padding: .4em .2em;
}

div.input_area {
    margin: .2em 0.4em;
    max-width: 580px;
}

table.dataframe {
    font-family: Arial, sans-serif;
    font-size: 13px;
    line-height: 20px;
}

table.dataframe th, td {
    padding: 4px;
    text-align: left;
}

pre code {
    background-color: inherit;
}</style>

        </div>

        <footer class="disclaimer">
          <div class="container-fluid">
            <p>
              ¬© 2012-2017 Pybonacci, licencia <a href="https://github.com/Pybonacci/pybonacci.github.io/blob/sources/LICENSE.md"> CC BY-SA 4.0 + MIT</a>
              salvo otra indicaci√≥n.
              <p>Contenido generado con <a href= "http://docs.getpelican.com/">Pelican</a>.</p>
            </p>
          </div>
        </footer>

    </body>
</html>